\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
%\usepackage{booktabs}
\usepackage{array}
\usepackage[utf8]{inputenc}



\usepackage{epstopdf}
\usepackage{caption}


\usepackage[fleqn]{amsmath}



\usepackage{tipx}
\usepackage{tipa}

\usepackage{breakcites}
%\usepackage{/usr/local/texlive/2020/texmf-dist/tex/latex/breakcites/breakcites}

%\usepackage{supertabular}
%\usepackage{wasysym}

%\usepackage{setspace}

%\usepackage{pifont}



\usepackage{enumitem}

\usepackage{float} %%%% use H! position -- conserve vertical space

%\usepackage{mathabx}


%\usepackage{txfonts}


%\usepackage{Sweave}

\usepackage{fancyvrb} %%% for \VerbatimInput


%%%%%%%%%%%%%%%%%%%%%%%% FOR HYPER REF
\usepackage{xcolor}
\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}
\definecolor{MyDarkRed}{rgb}{0.4,0.0,0.0} 
\definecolor{MyBlue}{rgb}{0.0, 0.0, 0.5} 

\definecolor{MyOrange1}{rgb}{1.0, 0.9, 0.0} 

\usepackage[colorlinks=true, urlcolor= MyDarkRed, linkcolor= MyBlue, citecolor=MyDarkGreen ]{hyperref}

%\usepackage[colorlinks=false, urlcolor= MyOrange1, linkbordercolor=MyOrange1, citecolor=MyDarkGreen ]{hyperref}

\usepackage{makeidx}

\usepackage{listings}
\lstdefinestyle{dm4ds_lstCustom_01}{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true
}
\input{_latexFormatting_01.tex}


\usepackage{geometry}

\usepackage{textcomp}
\usepackage{multirow}
\usepackage{float}

%\usepackage[colorlinks=true, urlcolor= \rgb{0.0,0.2,0.0}  ]{hyperref}




%\singlespacing
%\onehalfspacing
%\doublespacing

%\pagestyle{empty} %% turn off page numbering

\DeclareCaptionLabelSeparator{space}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\textwidth = 6.5 in
\textheight = 8.2 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.7 in
\parskip = 0.2in
\parindent = 0.0in
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
%\title{Brief Article}
%\author{The Author}



\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\makeindex

\begin{document}


%\newcommand\textcode\Verb


%\maketitle



\newif\ifuselocaldir
\uselocaldirtrue
\uselocaldirfalse


\newcommand{\DXZ}{
\begin{flushright}
\vspace{-.4in}
 { \raisebox{0.30ex}{{\tiny D}}\hspace{0.008in}X\hspace{0.01in}\raisebox{0.30ex}{{\tiny Z}}     }
\end{flushright}
}


\newenvironment{myQuote}[2]%
               {\begin{list}{}{\leftmargin#1\rightmargin#2}\item{}}%
               {\end{list}}

\begin{myQuote}{2cm}{2cm}
\begin{center}
{\huge
\textbf{CI Pathway: Parallel Computing} \\[0.4cm]
Assignment - 1
} \\[0.4cm]
\end{center}
\end{myQuote}

%\begin{myQuote}{3cm}{3cm}
%\begin{center}
%PRILIMINARY \& INCOMPLETE \\
%\end{center}
%\end{myQuote}



\begin{myQuote}{3cm}{3cm}
{\normalsize
\begin{center}
UCLA, Statistics\\Hochan Son\\Summer 2025\\
\today
%2012-03-12
\end{center}
}
\end{myQuote}


%\begin{abstract}
%{\normalsize
%Here's my abstract
%}
%\end{abstract}


%\newpage


%%\tableofcontents


%\newpage


%\input{_example.tex}

%\chapter{Introduction}

%Our textbook is \cite{briney2015data}.


%%%%%%%%%%%%%%%%




\section{Introduction}
This analysis evaluates parallel computing performance using the Laplace equation solver as a benchmark. The study compares four implementation approaches across varying thread/process counts to assess scalability and efficiency characteristics.

\section{Hardware Environment}

\subsection{System Specifications}

\begin{table}[H]
\centering
\caption{NCSA Delta Compute Environment}
\label{tab:system_specs}
\begin{tabular}{@{}ll@{}}
\hline
\textbf{Component} & \textbf{Specification} \\
\hline
Compute Platform & NCSA Delta HPC Cluster \\
Login Node & dt-login04.delta.ncsa.illinois.edu \\
Compute Node & cn034.delta.ncsa.illinois.edu \\
Operating System & Linux 4.18.0-477.95.1.el8\_8.x86\_64 \\
Distribution & Red Hat Enterprise Linux 8 \\
Architecture & x86\_64 \\
Node Interconnect & HPE Slingshot \\
\hline
\end{tabular}
\end{table}


\subsection{Processor Architecture}

\begin{table}[H]
\centering
\caption{AMD EPYC 7763 Processor Specifications}
\label{tab:processor_specs}
\begin{tabular}{@{}ll@{}}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
CPU Model & AMD EPYC 7763 64-Core Processor \\
Architecture & AMD Zen 3 (Milan) \\
Physical Cores & 64 per socket \\
Hardware Threads & 128 (2-way SMT) \\
Base Clock & 2.45 GHz \\
Boost Clock & Up to 3.5 GHz \\
Manufacturing Process & 7nm TSMC \\
Socket Type & SP3 \\
\hline
\end{tabular}
\end{table}



\section{Exercises For This Module.}
Our first exercises will be to compile, run and time the Laplace code using the two programming
models in this Pathway: \texttt{OpenMP and MPI}.
This will get you familiar with the programming environment in preparation for our actual parallel
programming in the next two modules.
It will also allow you to experience some parallel scaling first hand.
Specifically, our goal will be to compile the Laplace code in OpenMP and run it on varying
numbers of \texttt{threads} to see how much it speeds up.
We will do the same thing using MPI to run with multiple \texttt{processes}.
Don't worry if it seems like we are skipping over some details today - we are. But those details
will be made clear in our following modules.

%\newgeometry{top=1cm, left=1cm, right=1cm}
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=17.5cm]{_assets/bigStudentPicArray_01.jpeg}
%\caption{UCLA Graduate Statistics Students}
%\label{fig:studentArray}
%\end{center}
%\end{figure}
%\restoregeometry


\subsection{OpenMP}

\begin{enumerate}
  \item{Go into the OpenMP folder inside the Exercises folder. You will see codes there called \texttt{laplace\_omp.c} and \texttt{laplace\_omp.f90}. Select whichever language most interests you.
  
  To compile with OpenMP we do either:
    \begin{verbatim}
      nvc -mp laplace_omp.c
      or
      nvfortran -mp laplace_omp.f90
    \end{verbatim}
    
  Now we have an executable called a.out. But we need to ask for a compute node with multiple cores allocated in order to run. The Slurm command to get us a command line (--pty bash) on a compute node \texttt{(--nodes=1)} with 32 cores \texttt{(--cpus-per-task=32)} is:
    \begin{verbatim}
    srun --account=becs-delta-cpu --partition=cpu-interactive \
       --nodes=1 --cpus-per-task=32 --pty bash

    \end{verbatim}
  }

  \item {You will notice a few messages as Slurm find the resources, and then your command line will change to something with a compute node number in it. From the command line on this compute node we can run up to 32 cores. OpenMP allows us to control core usage with the \texttt{OMP\_NUM\_THREADS} environment variable. You learned about these in the Intro to Delta module. Set this variable to request 1 core \texttt{`( export `OMP\_NUM\_THREADS=1 )} and run with a.out to find the baseline run time for the Laplace code. If you select 4000 iterations, the code will run to a complete solution. It reports its own runtime for you. Now, try varying number of cores up to 32 and see what kind of speedup you experience. Note that you do not need to recompile the code. Just change the environment variable and run \texttt{a.out}. Record these times for our discussion. exit the compute node when you are finished. You should find yourself returned to the login node. }
\end{enumerate}

\subsection{MPI} 

\begin{enumerate}
  \item{Now we will do a similar exercise using MPI. Go into the MPI folder inside the Exercises folder. You will see codes there called \texttt{laplace\_mpi.c} and \texttt{laplace\_mpi.f}. Select whichever language most interests you.
  
  To compile with MPI we do either:
    \begin{verbatim}
      mpicc laplace_mpi.c
      or
      mpif90 laplace_mpi.f90
    \end{verbatim}
  
  Again, you have an executable called a.out. Now you need to ask for a compute node with multiple processes allocated in order to run. Similar, but not identical, to the previous Slurm command, the one to get us a command line on a compute node with 4 processes \texttt{(--nodes=1 --tasks=4 --tasks-per-node=4)} is:
  \begin{verbatim}
    srun --account=becs-delta-cpu --partition=cpu-interactive \
    --nodes=1 --tasks=4 --tasks-per-node=4 --pty bash
  \end{verbatim}
  }
  \item{With MPI we will limit ourselves to a single timing run, using 4 processes. The command to
  run our a.out executable on our four available processes is
  \begin{verbatim}
    mpirun -n 4 a.out
  \end{verbatim}
  Record this time for our later discussion.
  Exit the compute node when you are finished.}
\end{enumerate}

\section {Exercise}

\begin{enumerate}
  \item{Exercise 1: Compile, run, and time the Laplace code using the two programming models in this Pathway: OpenMP and MPI.}
    Vary the number of cores up to 32 by changing the \texttt{OMP\_NUM\_THREADS} environment variable to see what speedup you experience. Record these times.


  \item{Exercise 2: Execute a single timing run, using 4 processes, and record the time obtained.}

\end{enumerate}



\section {Solutions: OpenMP and MPI Performance Analysis}

\subsection {Serial, OpenMP, and Enhanced Parallel Processing}
  The purpose of this exerise to measure the differeneces how the parallelism benefits the speed of the execution by distributed computation over multiple cpu cores. I've done few observations.
  1. varying threads (1,8, and 32) on \texttt{OMP\_NUM\_THREADS} counts. 
  2. varying techniques such as serial, openmp, and red-black (checkerboard algorithm) by complier tuning. 
  The performance evaluation reveals significant differences across parallelization approaches, as summarized in the following tables.\\
  Each of the process has completed with 1,8, and 32 threads. It has shown that the serial process has no performance improvement across threads counts. The OpenMP has shown good performance. Although Enhanced Parallel test with red-black checker board algorithm has the best outcome at threads=32. 
  However, the efficiency on 32 threads has decreasing. As thread count increases, efficiency typically decreases due to parallel overhead, communication costs, and workload imbalances.

  \begin{table}[H]
  \centering
  \caption{Parallel Processing Performance Comparison}
  \label{tab:performance_comparison}
  \footnotesize
  \begin{tabular}{lccccc}
  \hline
  \textbf{Method} & \textbf{Threads} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency (\%)} & \textbf{Iterations} \\
  \hline
  Serial Process Test & 1  & 22.040 & 1.00$\times$ & 100.0 & 3372 \\
  Serial Process Test & 8  & 22.037 & 1.00$\times$ & 12.5  & 3372 \\
  Serial Process Test & 32 & 22.065 & 1.00$\times$ & 3.1   & 3372 \\
  \hline
  OpenMP Process Test & 1  & 21.733 & 1.00$\times$ & 100.0 & 3372 \\
  OpenMP Process Test & 8  & 4.175  & 5.21$\times$ & 65.1  & 3372 \\
  OpenMP Process Test & 32 & 1.992  & 10.91$\times$ & 34.1 & 3372 \\
  \hline
  Enhanced (Red/Black) Parallel Test & 1  & 12.738 & 1.00$\times$ & 100.0 & 3279 \\
  Enhanced (Red/Black) Parallel Test & 8  & 2.569  & 4.96$\times$ & 62.0  & 3279 \\
  Enhanced (Red/Black) Parallel Test & 32 & 1.490  & 8.55$\times$ & 26.7  & 3279 \\
  \hline
  \end{tabular}
  \end{table}
  
  \begin{itemize}
    \item \textbf{Serial Test}: Shows no parallelization benefit, confirming serial execution regardless of thread count.
    
    \item \textbf{OpenMP Implementation}: 
    \\Demonstrates significant speedup with increasing thread count:
      \begin{itemize}
      \item 8 threads: 5.21$\times$ speedup (65.1\% efficiency)
      \item 32 threads: 10.91$\times$ speedup (34.1\% efficiency)
      \end{itemize}
    
    \item \textbf{Enhanced Red/Black (checkerboard algorithm) Method}: Achieves best overall performance:
      \begin{itemize}
      \item Superior single-thread performance (12.738s vs 21.733s)
      \item Best 32-thread performance (1.490s vs 1.992s)
      \item 25.2\% faster than OpenMP at 32 threads
      \item Slight reduction in required iterations (3279 vs 3372)
      \end{itemize}
    
    \item \textbf{Efficiency Analysis}: Both parallel methods show decreasing efficiency with higher thread counts, typical of parallel overhead and diminishing returns.
  
  \end{itemize}

\subsection {Serial, MPI Processing}

The MPI implementation of the Laplace code demonstrates a different approach to parallelization compared to OpenMP. The key observations from the MPI results are as follows:
\begin{table}[H]
\centering
\caption{MPI vs Serial Process Performance Comparison}
\label{tab:mpi_performance}
\begin{tabular}{lccccc}
\hline
\textbf{Test Type} & \textbf{Processes} & \textbf{Time (s)} & \textbf{Speedup} & \textbf{Efficiency (\%)} & \textbf{Status} \\
\hline
\multirow{2}{*}{\shortstack{Serial Process\\Test}} 
& 1  & 46.711 & 1.00$\times$ & 100.0 & \textcolor{green}{Success} \\
& 4  & 21.878 & 2.14$\times$ & 53.5  & \textcolor{orange}{Partial}$^*$ \\
\hline
\multirow{2}{*}{\shortstack{MPI\\Test}} 
& 1  & -- & -- & -- & \textcolor{red}{Failed}$^{**}$ \\
& 4  & 6.397  & 7.30$\times$ & 182.6 & \textcolor{green}{Success} \\
\hline
\end{tabular}
\end{table}
\begin{footnotesize}
\footnotesize{
$^*$ 3 out of 4 processes terminated immediately (0 iterations) \\
$^{**}$ Error: "This code must be run with 4 PEs"
}
\end{footnotesize}
\\The MPI implementation of the Laplace code provides a unique perspective on parallel processing, particularly in how it handles process management and performance scaling. The key findings from the MPI results are:
\begin{itemize}
  \item \textbf{MPI achieves 7.30$\times$ speedup} with 4 processes, demonstrating excellent parallel scaling
  \item \textbf{Super-linear efficiency} The super-linear efficiency of 182.6\% suggests measurement inconsistencies between MPI and serial baselines, likely due to different optimization levels in the implementations rather than true performance gains.
  \item \textbf{Serial 4-process anomaly}: When serial code runs with 4 processes, 3 terminate immediately while 1 completes the work
\end{itemize}

\vspace{1em}

\begin{table}[H]
\centering
\caption{MPI Implementation Analysis}
\label{tab:mpi_analysis}
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Value} & \textbf{Interpretation} \\
\hline
Baseline Performance (1P) & 46.711 s & Significantly slower than OpenMP baseline \\
MPI Performance (4P) & 6.397 s & Strong parallel scaling \\
Speedup (1P → 4P) & 7.30$\times$ & Excellent parallelization \\
Parallel Efficiency & 182.6\% & Super-linear efficiency$^{\dagger}$ \\
\hline
\textbf{Key Ratios} & & \\
Serial 4P vs Serial 1P & 2.14$\times$ faster & Unexpected improvement \\
MPI 4P vs Serial 4P & 3.42$\times$ faster & True MPI benefit \\
\hline
\end{tabular}
\end{table}
\vspace{0.5em}
\begin{footnotesize}
\footnotesize{
$^{\dagger}$ Super-linear efficiency suggests different baseline implementation
}
\end{footnotesize}

The MPI implementation demonstrates strong parallel scaling with a 7.30× speedup using 4 processes, but reveals several critical implementation characteristics. The MPI version is hard-coded to require exactly 4 processes and fails completely with any other configuration. Most notably, the MPI serial baseline (46.7s) is significantly slower than comparable OpenMP implementations, suggesting different algorithmic implementations or optimization levels. When the serial code is forced to run with 4 processes, 3 processes terminate immediately while only 1 completes the work in 21.9s, indicating the serial version lacks proper MPI coordination. The true MPI parallelization benefit is 3.42× over the functioning 4-process baseline, which represents more realistic performance gains. While MPI achieves good parallel efficiency (85.5\% based on realistic baseline), it remains less flexible than OpenMP due to its fixed process requirement and shows inferior absolute performance compared to OpenMP's 8-thread implementation (6.4s vs 4.2s).
%\include{appendicitis}

\section*{Conclusion}

This performance analysis reveals distinct characteristics across three computational paradigms for iterative numerical methods. \textbf{Serial execution} provides simplicity and deterministic behavior but is fundamentally limited by single-core performance (22.0-46.7s baseline). \textbf{OpenMP shared-memory parallelization} demonstrates excellent scalability with 10.91× speedup using 32 threads, offering the best balance of performance (1.992s), programming simplicity, and flexibility, though efficiency degrades from 65.1\% (8 threads) to 34.1\% (32 threads) due to memory bandwidth limitations and synchronization overhead. \textbf{MPI distributed-memory parallelization} achieves strong parallel scaling (7.30× speedup with 4 processes) and enables cluster computing, but suffers from implementation rigidity (hard-coded to exactly 4 processes), communication overhead, and reduced absolute performance (6.397s) compared to OpenMP.

The \textbf{Enhanced Red/Black checkerboard algorithm} proves that algorithmic optimization can be more impactful than parallelization paradigm choice alone. This approach delivers 25-41\% performance improvements across all configurations, reducing single-thread baseline from 21.7s to 12.7s and achieving superior 32-thread performance (1.490s vs 1.992s). The Red/Black method eliminates data dependencies through spatial decomposition, accelerates convergence (2.8\% fewer iterations), and enables better cache utilization. Combined with OpenMP's 32-thread parallelization, it achieves the optimal performance of 1.490s with 14.80× speedup, demonstrating that \textbf{hybrid approaches combining algorithmic innovation with appropriate parallelization strategies yield the most significant performance gains} for computationally intensive iterative methods.

For practical applications, \textbf{OpenMP with 8-16 threads provides the optimal efficiency balance (45-65\%)}, while the Red/Black algorithm should be adopted universally for its consistent performance benefits. Future development should pursue \textbf{hybrid MPI+OpenMP implementations with Red/Black optimization} to combine distributed scalability with optimal single-node performance, leveraging the strengths of each paradigm while mitigating their individual limitations.


%\newpage{}
%\pagebreak{}

\bibliographystyle{plain}

\bibliography{Lab_X} 


\newpage

\section{Appendix.code}

Here's some of our code (Note the use of VerbatimInput from package \texttt{fancyvrb}):


\subsection{Code A: laplace\_serial.c}

%\begin{footnotesize}
%\VerbatimInput{_code_A.R}
%\end{footnotesize}

\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{ex2/laplace_serial.c}
\end{footnotesize}




\subsection{Code B: laplace\_omp.c}

%\begin{footnotesize}
%\VerbatimInput{_code_B.R}
%\end{footnotesize}

\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{ex1/laplace_omp.c}
\end{footnotesize}




\subsection{Code C: laplace\_omp\_parallel.c}

%\begin{footnotesize}
%\VerbatimInput{_code_C.R}
%\end{footnotesize}

\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{ex1/laplace_omp_parallel.c}
\end{footnotesize}


\subsection{Code D: laplace\_mpi.c}

%\begin{footnotesize}
%\VerbatimInput{_code_C.R}
%\end{footnotesize}

\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{ex2/laplace_mpi.c}
\end{footnotesize}



\section{Appendix.results}

\subsection{ex1\_result.txt}
\begin{footnotesize}
 \lstinputlisting[style=dm4ds_lstCustom_01]{./ex1_result.txt}
\end{footnotesize}

\subsection{ex2\_result.txt}
\begin{footnotesize}
 \lstinputlisting[style=dm4ds_lstCustom_01]{ex2/ex2_result.txt}
\end{footnotesize}


\end{document}