\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amssymb}
%\usepackage{booktabs}
\usepackage{array}
\usepackage[utf8]{inputenc}



\usepackage{epstopdf}
\usepackage{caption}


\usepackage[fleqn]{amsmath}



\usepackage{tipx}
\usepackage{tipa}

\usepackage{breakcites}
%\usepackage{/usr/local/texlive/2020/texmf-dist/tex/latex/breakcites/breakcites}

%\usepackage{supertabular}
%\usepackage{wasysym}

%\usepackage{setspace}

%\usepackage{pifont}



\usepackage{enumitem}

\usepackage{float} %%%% use H! position -- conserve vertical space

%\usepackage{mathabx}


%\usepackage{txfonts}


%\usepackage{Sweave}

\usepackage{fancyvrb} %%% for \VerbatimInput


%%%%%%%%%%%%%%%%%%%%%%%% FOR HYPER REF
\usepackage{xcolor}
\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0}
\definecolor{MyDarkRed}{rgb}{0.4,0.0,0.0} 
\definecolor{MyBlue}{rgb}{0.0, 0.0, 0.5} 

\definecolor{MyOrange1}{rgb}{1.0, 0.9, 0.0} 

\usepackage[colorlinks=true, urlcolor= MyDarkRed, linkcolor= MyBlue, citecolor=MyDarkGreen ]{hyperref}

%\usepackage[colorlinks=false, urlcolor= MyOrange1, linkbordercolor=MyOrange1, citecolor=MyDarkGreen ]{hyperref}

\usepackage{makeidx}

\usepackage{listings}
\lstdefinestyle{dm4ds_lstCustom_01}{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true
}
\input{_latexFormatting_01.tex}


\usepackage{geometry}

\usepackage{textcomp}
\usepackage{multirow}
\usepackage{float}

%\usepackage[colorlinks=true, urlcolor= \rgb{0.0,0.2,0.0}  ]{hyperref}




%\singlespacing
%\onehalfspacing
%\doublespacing

%\pagestyle{empty} %% turn off page numbering

\DeclareCaptionLabelSeparator{space}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\textwidth = 6.5 in
\textheight = 8.2 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.7 in
\parskip = 0.2in
\parindent = 0.0in
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
%\title{Brief Article}
%\author{The Author}



\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\makeindex

\begin{document}


%\newcommand\textcode\Verb


%\maketitle



\newif\ifuselocaldir
\uselocaldirtrue
\uselocaldirfalse


\newcommand{\DXZ}{
\begin{flushright}
\vspace{-.4in}
 { \raisebox{0.30ex}{{\tiny D}}\hspace{0.008in}X\hspace{0.01in}\raisebox{0.30ex}{{\tiny Z}}     }
\end{flushright}
}


\newenvironment{myQuote}[2]%
               {\begin{list}{}{\leftmargin#1\rightmargin#2}\item{}}%
               {\end{list}}

\begin{myQuote}{2cm}{2cm}
\begin{center}
{\huge
\textbf{CI Pathway: Parallel Computing} \\[0.4cm]
Assignment: Shared Memory Parallelism with OpenMP
} \\[0.4cm]
\end{center}
\end{myQuote}

%\begin{myQuote}{3cm}{3cm}
%\begin{center}
%PRILIMINARY \& INCOMPLETE \\
%\end{center}
%\end{myQuote}



\begin{myQuote}{3cm}{3cm}
{\normalsize
\begin{center}
UCLA, Statistics\\Hochan Son\\Summer 2025\\
\today
%2012-03-12
\end{center}
}
\end{myQuote}


%\begin{abstract}
%{\normalsize
%Here's my abstract
%}
%\end{abstract}


%\newpage


%%\tableofcontents


%\newpage


%\input{_example.tex}

%\chapter{Introduction}

%Our textbook is \cite{briney2015data}.


%%%%%%%%%%%%%%%%




\section{Introduction}
This study examines the performance characteristics and optimization strategies of OpenMP-based
parallel programming for computational prime number generation. Through systematic analysis of
four distinct implementation approaches, we investigate how different parallelization strategies and
algorithmic optimizations impact computational efficiency and scalability.

The primary learning objectives of this OpenMP exercise include:
\begin{enumerate}
  \item{Understanding race condition avoidance in shared-memory parallel programming}
  \item{Comparing different OpenMP synchronization mechanisms (atomic operations, critical sections, and reductions)}
  \item{Analyzing the relationship between algorithmic complexity and parallelization benefits}
  \item{Evaluating parallel efficiency and scalability across varying thread counts}
  \item{Demonstrating the fundamental principle that algorithmic optimization often provides greater 
  performance gains than parallelization alone}
\end{enumerate}

The exercise employs a computationally intensive prime number generation task to stress-test
different OpenMP implementations and reveal their relative strengths and limitations under high-
performance computing conditions.

\section{Hardware Environment}

\subsection{System Specifications}
The experiments were conducted on the NCSA Delta HPC cluster, which is a high-performance computing environment designed for parallel processing tasks. The specifications of the system are as follows:
\begin{table}[H]
\centering
\caption{NCSA Delta Compute Environment}
\label{tab:system_specs}
\begin{tabular}{@{}ll@{}}
\hline
\textbf{Component} & \textbf{Specification} \\
\hline
Compute Platform & NCSA Delta HPC Cluster \\
Login Node & dt-login04.delta.ncsa.illinois.edu \\
Compute Node & cn096.delta.ncsa.illinois.edu \\
Operating System & Linux 4.18.0-477.95.1.el8\_8.x86\_64 \\
Distribution & Red Hat Enterprise Linux 8 \\
Architecture & x86\_64 \\
Node Interconnect & HPE Slingshot \\
\hline
\end{tabular}
\end{table}


\subsection{Processor Architecture}

\begin{table}[H]
\centering
\caption{AMD EPYC 7763 Processor Specifications}
\label{tab:processor_specs}
\begin{tabular}{@{}ll@{}}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
CPU Model & AMD EPYC 7763 64-Core Processor \\
Architecture & AMD Zen 3 (Milan) \\
Physical Cores & 64 per socket \\
Hardware Threads & 128 (2-way SMT) \\
Base Clock & 2.45 GHz \\
Boost Clock & Up to 3.5 GHz \\
Manufacturing Process & 7nm TSMC \\
Socket Type & SP3 \\
L3 Cache & 256 MB \\
Memory Support & DDR4-3200 \\
\hline
\end{tabular}
\end{table}



\section{Exercises Methodology}

%\newgeometry{top=1cm, left=1cm, right=1cm}
%\begin{figure}[htbp]
%\begin{center}
%\includegraphics[width=17.5cm]{_assets/bigStudentPicArray_01.jpeg}
%\caption{UCLA Graduate Statistics Students}
%\label{fig:studentArray}
%\end{center}
%\end{figure}
%\restoregeometry

\subsection{OpenMP Exercise}

\begin{enumerate}
  \item{You will find this code in \texttt{\/projects\/becs\/urbanic} as either \texttt{prime\_serial.c or prime\_serial.f}.
  
  Your job is to accelerate this code using OpenMP. You should see a dramatic speedup if you use our OpenMP directives effectively.
  
  If you are not careful, you could introduce a race condition and have inconsistent results. If you use the same caution we used in the examples above, you will avoid this.
  
  To reiterate what you did in the previous module.
  }
  \begin{enumerate}
    \item {Compile with something like
    \begin{verbatim}
      nvc -mp prime_serial.c
      or
      nvfortran -mp prime_serial.f
    \end{verbatim}
    }
    \item {Grab multiple cores on a compute node with something like
    \begin{verbatim}
      srun --account=becs-delta-cpu --partition=cpu-interactive \
      --nodes=1 --cpus-per-task=32 --pty bash
    \end{verbatim}
    }
    \item {Set the number of threads you wish to run with using something like
    \begin{verbatim}
        export OMP_NUM_THREADS=16
    \end{verbatim}
    }
    Submit your code and your timings for at least 1, 4, 8, 16 and 32 threads.
  \end{enumerate}

\end{enumerate}

\section{Solution to OpenMP Exercise}

\subsection{Method 0: Serial Code}

The serial code for prime number generation is a straightforward implementation that checks each number for primality by testing divisibility against all smaller numbers. This method is inherently sequential and does not leverage parallel processing capabilities.

\begin{verbatim}
for (i = 2; i <= n; i++) {
  for (j = 2; j < i; j++) {
    if (i % j == 0) {
      #pragma omp atomic
      not_primes++;
      break;
    }
  }
}
\end{verbatim}

\subsection{Method 1: Atomic Operations}

Using atomic operations for thread-safe updates to shared variables is a common approach in OpenMP. This method ensures that only one thread can update the shared variable at a time

\begin{verbatim}
#pragma omp parallel for private(i,j)
for (i = 2; i <= n; i++) {
  for (j = 2; j < i; j++) {
    if (i % j == 0) {
      #pragma omp atomic
      not_primes++;
      break;
    }
  }
}
\end{verbatim}


\subsection{Method 2: Manual Reduction with Critical Sections}

This method uses a critical section to ensure that only one thread can update the shared variable at a time. While this approach is safe, it can lead to performance bottlenecks due to contention among threads.

\begin{verbatim}
#pragma omp parallel private(i,j,local_not_primes)
{
  local_not_primes = 0;

  #pragma omp for schedule(static)
  for (i = 2; i <= n; i++) {
    // computation...
    local_not_primes++;
  }
  #pragma omp critical
  {
    not_primes += local_not_primes;
  }
}

\end{verbatim}


\subsection{Method 3: OpenMP Reduction}

Leverages OpenMP's built-in reduction clause:

\begin{verbatim}
#pragma omp parallel for private(i,j) reduction(+:not_primes)
for (i = 2; i <= n; i++) {
    for (j = 2; j < i; j++) {
        if (i % j == 0) {
            not_primes++;
            break;
        }
    }
}
\end{verbatim}

\subsection{Method 4: Algorithmic Optimization}

Implements optimized trial division with reduced computational complexity, this method significantly reduces the number of iterations required to check for primality, especially for larger numbers. It skips even numbers and only checks divisibility up to the square root of each number.

\begin{verbatim}
#pragma omp parallel for private(i,j) reduction(+:not_primes)
for (i = 2; i <= n; i++) {
    if (i == 2) continue;
    if (i % 2 == 0) { not_primes++; continue; }
    
    int sqrt_i = (int)sqrt(i);
    for (j = 3; j <= sqrt_i; j += 2) {
        if (i % j == 0) {
            not_primes++;
            break;
        }
    }
}
\end{verbatim}


\section{Performance Analysis}

\subsection{Execution Time Results}

The execution times for each method across different thread counts are summarized in Table \ref{tab:execution_times}. The results demonstrate the performance improvements achieved through parallelization and algorithmic optimization.
\begin{table}[H]
\centering
\caption{Execution Time Performance Comparison (seconds)}
\label{tab:execution_times}
\footnotesize
\begin{tabular}{lrrrrr}
\hline
\textbf{Method} & \textbf{1 Thread} & \textbf{4 Threads} & \textbf{8 Threads} & \textbf{16 Threads} & \textbf{32 Threads} \\
\hline
Serial Baseline & 18.510 & 18.519 & 18.514 & 18.500 & 18.510 \\
Method 1 (Atomic) & 18.526 & 8.411 & 4.323 & 2.342 & 1.656 \\
Method 2 (Manual) & 24.462 & 10.384 & 5.555 & 3.008 & 1.878 \\
Method 3 (Reduction) & 20.161 & 8.147 & 4.701 & 2.510 & 1.523 \\
Method 4 (Optimized) & 0.035 & 0.019 & 0.015 & 0.021 & 0.037 \\
\hline
\end{tabular}
\end{table}

\subsection{Speedup Analysis}

The speedup factor relative to the serial baseline is calculated for each method across different thread counts. The results are presented in Table \ref{tab:speedup}. Method 4 demonstrates a revolutionary performance improvement due to its algorithmic optimization, achieving speedups of up to 1200$\times$ compared to the serial implementation. The result also shows that Methods 1-3 achieve significant speedups through parallelization, with Method 3 (OpenMP reduction) achieving the highest speedup among trial division methods.
\begin{table}[H]
\centering
\caption{Speedup Factor Relative to Serial Baseline}
\label{tab:speedup}
\footnotesize
\begin{tabular}{lrrrrr}
\hline
\textbf{Method} & \textbf{1 Thread} & \textbf{4 Threads} & \textbf{8 Threads} & \textbf{16 Threads} & \textbf{32 Threads} \\
\hline
Method 1 (Atomic) & 1.00$\times$ & 2.20$\times$ & 4.28$\times$ & 7.91$\times$ & 11.19$\times$ \\
Method 2 (Manual) & 0.76$\times$ & 1.78$\times$ & 3.33$\times$ & 6.15$\times$ & 9.86$\times$ \\
Method 3 (Reduction) & 0.92$\times$ & 2.27$\times$ & 3.94$\times$ & 7.38$\times$ & 12.15$\times$ \\
Method 4 (Optimized) & 528$\times$ & 974$\times$ & 1234$\times$ & 881$\times$ & 500$\times$ \\
\hline
\end{tabular}
\end{table}

\subsection{Parallel Efficiency}

The parallel efficiency is calculated as the ratio of the speedup factor to the number of threads used. This metric provides insight into how effectively the parallelization scales with increasing thread counts. The results are summarized in Table \ref{tab:efficiency}. Methods 1-3 maintain good efficiency up to 32 threads, while Method 4 shows diminishing returns due to its extremely fast execution time.

\begin{table}[H]
\centering
\caption{Parallel Efficiency (\%)}
\label{tab:efficiency}
\footnotesize
\begin{tabular}{lrrrrr}
\hline
\textbf{Method} & \textbf{1 Thread} & \textbf{4 Threads} & \textbf{8 Threads} & \textbf{16 Threads} & \textbf{32 Threads} \\
\hline
Method 1 (Atomic) & 100.0 & 55.0 & 53.5 & 49.4 & 35.0 \\
Method 2 (Manual) & 100.0 & 58.6 & 54.9 & 50.6 & 40.6 \\
Method 3 (Reduction) & 100.0 & 56.8 & 49.3 & 46.1 & 38.0 \\
Method 4 (Optimized) & 100.0 & 24.4 & 15.4 & 5.5 & 1.6 \\
\hline
\end{tabular}
\end{table}

\subsection{Performance Analysis Discussion}

\subsubsection{Trial Division Methods (1-3)}

The three trial division implementations demonstrate excellent parallel scalability:

\begin{itemize}
    \item \textbf{Method 1 (Atomic Operations)}: This method achieves the best balance of performance and simplicity, with 11.19$\times$ speedup at 32 threads and maintaining good efficiency (35\%).
    
    \item \textbf{Method 2 (Manual Reduction)}: The manual reduction method shows higher single-thread overhead due to OpenMP parallel region setup, but achieves comparable scalability with 9.86$\times$ speedup at 32 threads.
    
    \item \textbf{Method 3 (OpenMP Reduction)}: The OpenMP Reduction method demonstrates the highest absolute speedup (12.15$\times$) among trial division methods, validating the efficiency of OpenMP's built-in reduction implementation.
\end{itemize}

\subsubsection{Algorithmic Optimization (Method 4)}

Method 4 represents a paradigm shift in performance optimization:

\begin{itemize}
    \item \textbf{Revolutionary Performance}: Achieves 500-1200$\times$ speedup over trial division methods
    \item \textbf{Algorithmic Complexity}: Reduces from O(n$^2$) to approximately O(n$\sqrt{n}$) complexity
    \item \textbf{Limited Parallelization Benefits}: Shows decreasing efficiency with higher thread counts due to extremely fast execution times where overhead dominates
    \item \textbf{Optimal Thread Count}: Peak performance at 8 threads (0.015s) demonstrates the importance of matching thread count to problem characteristics
\end{itemize}

\subsubsection{Key Performance Insights}

\begin{enumerate}
    \item \textbf{Algorithm vs. Parallelization}: The 500-1200$\times$ improvement from algorithmic optimization far exceeds the 10-12$\times$ gains from parallelization, reinforcing the principle that algorithmic efficiency is paramount.
    
    \item \textbf{Synchronization Overhead}: All trial division methods show similar scalability patterns, indicating that synchronization strategy has minimal impact compared to computational complexity.
    
    \item \textbf{Scalability Limits}: Method 4's efficiency degradation at high thread counts demonstrates that not all algorithms benefit from aggressive parallelization.
    
    \item \textbf{Hardware Utilization}: The AMD EPYC 7763's 64-core architecture is well-utilized by Methods 1-3, but Method 4's speed makes thread overhead the limiting factor.
\end{enumerate}

\section{Conclusions}

This comprehensive analysis of OpenMP parallelization strategies yields several critical insights for high-performance computing. The results demonstrate that while parallelization can yield significant performance improvements, the most substantial gains come from algorithmic optimizations that reduce computational complexity.
While this exercise parallel programming is very complex in terms of the number of threads, synchronization strategies, and algorithmic optimizations. As such, the parallel debugging is also very complex. The baseline result from serial code was essential to validate the correctness of parallel implementations.

\subsection{Primary Findings}

\begin{enumerate}
    \item \textbf{Algorithmic Supremacy}: The most significant performance improvement (500-1200$\times$) came from algorithmic optimization rather than parallelization techniques, reinforcing the fundamental principle that efficient algorithms are the foundation of high-performance computing.
    
    \item \textbf{OpenMP Strategy Effectiveness}: Among parallelization approaches, OpenMP reduction (Method 3) achieved the highest speedup (12.15$\times$), followed closely by atomic operations (Method 1), demonstrating the efficiency of OpenMP's built-in parallel constructs.
    
    \item \textbf{Scalability Characteristics}: Trial division methods (1-3) exhibit excellent linear scaling up to 32 threads with 35-40\% efficiency, while the optimized algorithm shows limited parallelization benefits due to its inherently fast execution.
    
    \item \textbf{Race Condition Solutions}: All three synchronization strategies (atomic, critical sections, reduction) successfully eliminated race conditions while maintaining good parallel performance, with reduction showing slight superiority.
\end{enumerate}

%\newpage{}
%\pagebreak{}

\bibliographystyle{plain}

\bibliography{Lab_X} 


\newpage

\section{Appendix.code}

Here's some of our code (Note the use of VerbatimInput from package \texttt{fancyvrb}):


\subsection{Code A: prime\_serial.c}

\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{./prime_serial.c}
\end{footnotesize}


\subsection{Code B: prime\_parallel\_norace\_1.c}


\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{./prime_parallel_norace_1.c}
\end{footnotesize}


\subsection{Code C: prime\_parallel\_norace\_2.c}


\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{./prime_parallel_norace_2.c}
\end{footnotesize}


\subsection{Code D: prime\_parallel\_norace\_3.c}

\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{./prime_parallel_norace_3.c}
\end{footnotesize}


\subsection{Code D: prime\_parallel\_norace\_4.c}

\begin{footnotesize}
\lstinputlisting[style=dm4ds_lstCustom_01]{./prime_parallel_norace_4.c}
\end{footnotesize}


\section{Appendix.hw2\_result.txt}

\subsection{result.txt}

\begin{footnotesize}
 \lstinputlisting[style=dm4ds_lstCustom_01]{./hw2_result.txt}
\end{footnotesize}

\end{document}